## Vulnerability Name

**Author(s):**

David Rowe

**Description:**

LLM is not properly isolated when it has access to external resources or sensitive systems

**Common Examples of Vulnerability:**

1. Example 1: LLM has access to sensitive server or data information without restrictions
2. Example 2: LLM can consume data  live from websites and return the information on the output
3. Example 3: LLM can consume sensitive company information and use it in its tuning model

**How to Prevent:**

1. Prevention Step 1: A step or strategy that can be used to prevent the vulnerability or mitigate its effects.
2. Prevention Step 2: Prevent LLM from having public internet access.  LLM to stay on a private network
3. Prevention Step 3: Identify sensitive information patterns on input and prevent saving that information in the chaining/sequencing

**Example Attack Scenarios:**

Scenario #1: A person asks the chatbot for information while pretending to be someone with privilege vs having to be authorized to get the information.


**Reference Links:**

1. [Employees Are Feeding Sensitive Biz Data to ChatGPT, Raising Security Fears](https://www.darkreading.com/risk/employees-feeding-sensitive-business-data-chatgpt-raising-security-fears): More than 4% of employees have put sensitive corporate data into the large language model, raising concerns that its popularity may result in massive leaks of proprietary information.

2. [Link Title](URL): Brief description of the reference link.

**Author Commentary (Optional):**

(Optional) Any additional insights, opinions, or perspectives from the author that are relevant to understanding or addressing the vulnerability.
